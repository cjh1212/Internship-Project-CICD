name: "Data collection and train"

on:
  push:
    branches:
      - main
    
jobs:
  data_collection:
    name: data_collection
    runs-on: ubuntu-latest
    steps:
      - name: Check
        uses: actions/checkout@v3
      - name: python_packages
        run: python -m pip install --upgrade pip
          pip install pandas sklearn transformers datasets Beautifulsoup4
      - name: collection
        run: python3 collection.py
        
  train:
    name: train
    runs-on: ubuntu-latest
    needs: data_collection
    permissions:
      contents: 'read'
      id-token: 'write'
    steps:
      - name: Check
        uses: actions/checkout@v3
      - id: 'auth'
        uses: 'google-github-actions/auth@v0'
        with:
          credentials_json: '${{ secrets.gcp_credentials }}'
      - id: 'upload_data'
        uses: 'google-github-actions/upload-cloud-storage@v0'
        with:
          path: './train_data.csv'
          destination: 'bucket_cjh980803'
      - name: Set up cloud SDK
        uses: google-github-actions/setup-gcloud@v0
      - run: |-
          gsutil cp gs://bucket_cjh980803/train_data.csv .
          gcloud compute copy-files train_data.csv instance:~/ --zone=asia-northeast3-a
      - id: 'compute-ssh'
        uses: 'google-github-actions/ssh-compute@v0'
        with:
          instance_name: 'instance'
          zone: 'asia-northeast3-a'
          ssh_private_key: '$ {{ secrets.GCP_SSH_PRIVATE_KEY }}'
      - id: 'train'
        run: |-
          cd ../runner
          python3 preprocessing_train.py
        
          
        

